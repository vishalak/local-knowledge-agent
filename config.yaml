# === Local Knowledge Assistant - Configuration ===
# Change source_dir to the root folder containing your code and docs.
source_dir: "C:/vk-data"

# Where to store the local Chroma vector database (relative or absolute path)
chroma_path: "./chroma_db"

# Name of your vector collection
collection: "local_kb"

# File types to include
include_extensions:
  - ".md"
  - ".txt"
  - ".py"
  - ".ipynb"
  - ".js"
  - ".jsx"
  - ".ts"
  - ".tsx"
  - ".cs"          # C# source files
  - ".csproj"      # C# project files
  - ".sln"         # Visual Studio solution files
  - ".xaml"        # XAML markup files
  - ".razor"       # Razor pages
  - ".cshtml"      # Razor views
  - ".java"
  - ".kt"
  - ".go"
  - ".rb"
  - ".rs"
  - ".cpp"
  - ".c"
  - ".h"
  - ".hpp"
  - ".sh"
  - ".bash"
  - ".ps1"
  - ".yaml"
  - ".yml"
  - ".json"
  - ".ini"
  - ".toml"

# Folders to skip while scanning
excludes:
  - ".git"
  - "node_modules"
  - ".venv"
  - "__pycache__"
  - "build"
  - "dist"
  - ".terraform"
  - ".idea"
  - ".vscode"
  - ".vs"
  - "TestResults"
  - "out"
  - "logs"
  - "tmp"
  - "obj"
  - "bin"
  - "venv"
  - "packages"

# Chunking parameters
chunk:
  size: 1200        # characters per chunk
  overlap: 200      # overlap between chunks
  semantic: true    # use semantic chunking for better content grouping

# Metadata extraction settings
metadata:
  extract_advanced: true     # extract detailed file and content metadata
  generate_summaries: false  # generate AI summaries (requires Ollama, slower)

# Retrieval and query enhancement settings
retrieval:
  query_transform: "hyde"    # Query transformation method: "none", "hyde", "expand", "enhance", "multi"
  max_results: 10           # Maximum results to consider before filtering
  rerank_method: "bm25"     # Reranking method: "none", "bm25", "semantic", "hybrid"
  rerank_max_results: 5     # Maximum results after reranking
  vector_weight: 0.7        # Weight for vector similarity in hybrid reranking
  bm25_weight: 0.3          # Weight for BM25 score in hybrid reranking
  bm25_k1: 1.2              # BM25 parameter k1
  bm25_b: 0.75              # BM25 parameter b

# Chat and conversation settings
chat:
  enabled: true             # Enable conversational mode
  max_history: 10           # Maximum conversation exchanges to remember
  use_context: true         # Use conversation context for query enhancement
  max_context_exchanges: 3  # Maximum previous exchanges to include in context

model:
  embedder: "BAAI/bge-small-en-v1.5"   # local embedding model (downloaded once)
  llm: "llama3:8b"                     # Ollama model name (change as you like)
